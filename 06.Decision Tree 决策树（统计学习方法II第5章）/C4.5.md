## 信息熵（Entropy， $H$）

- 在信息论中，熵是接受的每条消息中包含的信息的平均量，又被称为信息熵，信源熵，平均自信息量。（熵最好理解为不确定性的量度，而不是确定性的量度，因为却随机的信源的上越大。）
- 熵表示事物不确定性的程度，也就是信息量的大小（一般说信息量大，就是指这个时候背后的不确定因素太多）

当取自有限的样本时，熵的公式可以表示为：
$$
Entropy = -\sum_{i=1}^{n}{p(x_i) * log_{b}{p(x_i)}}
$$
在这里$b$是对数所使用的底，通常是$2$，自然常数$e$，或是$10$。当$b=2$，熵的单位是$\rm bit$；当$b=e$，熵的单位是$\rm nat$；当$b=10$，熵的单位是$\rm Hart$。由定义可知，熵只依赖于X的分布，而与X的取值无关。



## 条件熵（conditional entropy）

在信息论中，条件熵描述了在已知第二个随机变量$X$的值的前提下，随机变量$Y$的信息熵还有多少。基于$X$条件的$Y$的信息熵，用$H(Y|X)$表示。如果$H(Y|X=x)$为变量$Y$在变量$X$取特定值$x$条件下的熵，那么$H(Y|X)$就是$H(Y|X=x)$在$X$取遍所有可能的$x$后取平均的结果。
$$
H(Y|X) = \sum_{i=1}^{n}{p_iH(Y|X=x_i)} \\
p_i = P(X=x_i), i=1, 2,...,n
$$








## C4.5

C4.5算法是ID3算法的一种延伸和优化，相比ID3，其改进点有：

1. 使用「信息增益率」来选择划分特征：
   - 优：克服了使用「信息增益」选择的不足
   - 缺：「信息增益率」对<u>可选值数目较少的属性</u>有所偏好
2. 可以处理①离散型和②<u>连续性（将连续性的属性离散化）</u>的属性类型
3. 能够处理具有<u>缺失属性值</u>的训练数据
4. 构造树的过程中进行了<u>剪枝</u>

### 特征选择

特征选择即选择一个最优划分属性，即从当前数据的特征中选择一个特征作为划分当前节点的标准。我们希望决策树的分支节点所包含的样本尽可能地属于同一个类别，即节点的‘纯度’越来越高

### 信息增益率（信息增益比）

「信息增益」对<u>可选值数目较多的属性</u>有所偏好，所以C4.5使用「信息增益率」来选择<u>最优划分属性</u>，公式：
$$
GainRatio(D|A) = \frac{infoGain(D|A)}{IV(A)}
\\
IV(A) = -\sum_{k=1}^K{\frac{|D_{k}|}{|D|}}*log_{2}{\frac{|D_{k}|}{|D|}}
$$
其中$A=[a_{1}, a_{2}, ...,a_{k}]$，表示A属性有K个可能的取值。若使用属性A类对数据集D进行划分，则会产生K个分支节点，其中第k个节点上包含A属性中的$a_{k}$个样本，记为$D_{k}$。通常，属性A可取值的数目越多（即K越大），则$IV(A)$的值通常会越大

> 可以理解，如果某个属性的可取值数目越少，则$GainRatio(D|A)$的分母会越小，则$GainRatio(D|A)$就会越大，则C4.5模型就会越倾向于选择该属性最为当前的最优划分属性

信息增益率准则对可取值数目较少的属性有所偏好。但是C4.5算法不是直接选择「信息增益率」最大的候选划分属性，而是先从候选划分属性中找出「信息增益」高于平均水平的属性，再从中选择「信息增益率」最高的

> 此点有些不明白，按照李航《统计学习方法》P78的说法，是直接选择「信息增益比」最大的特征。在此却需要过一层「信息增益高于平均水平」的筛选？

### 对连续特征处理

- 若当前属性为离散型，无需特殊处理

- 若当前属性为连续性，则对数据进行离散化处理，具体思路：

  1. 若数据集有M个样本，且属性A为连续性特征，那么按照A属性值的大小将样本进行排序：$a_{1}, a_{2},...a_{m}$。然后取相邻两样本的属性值得平均值作为候选切分点，不难算出共有M-1个候选切分点，其中第i个切分点$T_{i}$表示为：
     $$
     T_{i} = \frac{a_{i}+a_{i+1}}{2}
     $$

  2. 分别计算这M-1个候选切分点最为二元切分点时候的信息增益率，选择信息增益率最大的点作为该连续性特征的最佳切分点。比如取到的信息增益率最大的和切分点为$a_{t}$，则小于$a_{t}$的值为类别A1，大于$a_{t}$的值为类别A2，如此就做到了连续特征的离散化

### 对缺失值的处理

ID3不能处理缺失值，C4.5可以处理缺失值（常用概率权重方法），主要有三种情况：

1. 在有缺失值的特征上如何计算信息增益率？
   - 根据缺失比例，折算信息增益（无缺失值样本所占的比例乘以无缺失值样本子集的信息增益）和信息增益率
2. 选定了划分属性后，若某样本在该属性上的值是确实的，如何对这个样本进行划分？
   - 将样本以不同概率划分到不同节点中，概率是根据其他非缺失属性的比例得到的
3. infer时，测试样本特性有缺失值如何判断其类别？
   - 走所有分支，计算每个类别的概率，取概率最大的类别赋值给该样本

### 剪枝

剪枝（pruning）是为了缓解决策树的过拟合现象

剪枝又分为前剪枝很后剪枝，前剪枝是指在构造树的过程中就知道哪些节点可以剪掉。后剪枝是指构造出完整的决策树之后再考虑哪些子树可以剪掉





# 参考：

1. [熵（信息论）](https://zh.wikipedia.org/wiki/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA))
2. [决策树算法--C4.5算法](https://zhuanlan.zhihu.com/p/139188759)
3. 