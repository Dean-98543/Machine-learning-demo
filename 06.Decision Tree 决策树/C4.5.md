# C4.5

C4.5算法是ID3算法的一种延伸和优化，相比ID3，其改进点有：

1. 使用「信息增益率」来选择划分特征：
   - 优：克服了使用「信息增益」选择的不足
   - 缺：「信息增益率」对<u>可选值数目较少的属性</u>有所偏好
2. 可以处理①离散型和②<u>连续性（将连续性的属性离散化）</u>的属性类型
3. 能够处理具有<u>缺失属性值</u>的训练数据
4. 构造树的过程中进行了<u>剪枝</u>

## 特征选择

特征选择即选择一个最优划分属性，即从当前数据的特征中选择一个特征作为划分当前节点的标准。我们希望决策树的分支节点所包含的样本尽可能地属于同一个类别，即节点的‘纯度’越来越高

## 信息增益率（信息增益比）

「信息增益」对<u>可选值数目较多的属性</u>有所偏好，所以C4.5使用「信息增益率」来选择<u>最优划分属性</u>，公式：
$$
GainRatio(D|A) = \frac{infoGain(D|A)}{IV(A)}
\\
IV(A) = -\sum_{k=1}^K{\frac{|D_{k}|}{|D|}}*log_{2}{\frac{|D_{k}|}{|D|}}
$$
其中$A=[a_{1}, a_{2}, ...,a_{k}]$，表示A属性有K个可能的取值。若使用属性A类对数据集D进行划分，则会产生K个分支节点，其中第k个节点上包含A属性中的$a_{k}$个样本，记为$D_{k}$。通常，属性A可取值的数目越多（即K越大），则$IV(A)$的值通常会越大

> 可以理解，如果某个属性的可取值数目越少，则$GainRatio(D|A)$的分母会越小，则$GainRatio(D|A)$就会越大，则C4.5模型就会越倾向于选择该属性最为当前的最优划分属性

信息增益率准则对可取值数目较少的属性有所偏好。但是C4.5算法不是直接选择「信息增益率」最大的候选划分属性，而是先从候选划分属性中找出「信息增益」高于平均水平的属性，再从中选择「信息增益率」最高的

> 此点有些不明白，按照李航《统计学习方法》P78的说法，是直接选择「信息增益比」最大的特征。在此却需要过一层「信息增益高于平均水平」的筛选？

## 对连续特征处理

- 若当前属性为离散型，无需特殊处理

- 若当前属性为连续性，则对数据进行离散化处理，具体思路：

  1. 若数据集有M个样本，且属性A为连续性特征，那么按照A属性值的大小将样本进行排序：$a_{1}, a_{2},...a_{m}$。然后取相邻两样本的属性值得平均值作为候选切分点，不难算出共有M-1个候选切分点，其中第i个切分点$T_{i}$表示为：
     $$
     T_{i} = \frac{a_{i}+a_{i+1}}{2}
     $$

  2. 分别计算这M-1个候选切分点最为二元切分点时候的信息增益率，选择信息增益率最大的点作为该连续性特征的最佳切分点。比如取到的信息增益率最大的和切分点为$a_{t}$，则小于$a_{t}$的值为类别A1，大于$a_{t}$的值为类别A2，如此就做到了连续特征的离散化

## 对缺失值的处理

ID3不能处理缺失值，C4.5可以处理缺失值（常用概率权重方法），主要有三种情况：

1. 在有缺失值的特征上如何计算信息增益率？
   - 根据缺失比例，折算信息增益（无缺失值样本所占的比例乘以无缺失值样本子集的信息增益）和信息增益率
2. 选定了划分属性后，若某样本在该属性上的值是确实的，如何对这个样本进行划分？
   - 将样本以不同概率划分到不同节点中，概率是根据其他非缺失属性的比例得到的
3. infer时，测试样本特性有缺失值如何判断其类别？
   - 走所有分支，计算每个类别的概率，取概率最大的类别赋值给该样本

## 剪枝

剪枝（pruning）是为了缓解决策树的过拟合现象

剪枝又分为前剪枝很后剪枝，前剪枝是指在构造树的过程中就知道哪些节点可以剪掉。后剪枝是指构造出完整的决策树之后再考虑哪些子树可以剪掉





# 参考：

1. [决策树算法--C4.5算法](https://zhuanlan.zhihu.com/p/139188759)
2. 