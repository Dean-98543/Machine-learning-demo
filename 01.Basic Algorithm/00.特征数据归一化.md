# 特征数据归一化

为了消除数据特征之间不同量纲的影响，需要对特征进行归一化处理，使得不同指标之间具有可比性。对数值类型的特征做归一化可以将所有的特征统一到一个大致相同的数值区间内，使各个特征维度对目标函数的影响权重一致，从而提高迭代求解的收敛速度。最常用的方法有**线性函数归一化**和**零均值归一化**两种。

## 线形函数归一化（Min-Max Scaling）

线形函数归一化将原始数据进行线性变换，将原始数据映射到 $[0, 1]$ 的范围，实现对原始数据的等比缩放。线性函数归一化公式如下：
$$
X_{norm} = \frac{X-X_{min}}{X_{max} - X_{min}}
$$
其中，$X$ 为原始数据，$X_{max}$、$X_{min}$ 分别为数据的最大值和最小值。

**缺点：**抗干扰能力弱，首离群值影响较大，归一化后，特征中间容易没有数据。

## 零均值归一化（Z-Score Normalization）

零均值归一化会将原始数据映射到均值为 $0$，标准差为 $1$ 的分布上。具体来说，假设则零均值归一化公式如下：
$$
z = \frac{x-\mu}{\sigma}
$$
其中，$x$ 为原始数据，$\mu$ 和 $\sigma$ 分别为原始数据的均值和标准差。该归一化方法需求原始数据的分布近似为高斯分布，否则归一化的效果会变得糟糕。

**优点：**抗干扰能力强，和所有数据有关，求标准差需要所有数据介入，若有离群值，会被抑制下来。但是归一化后的数据最终结果不一定落在 $[0, 1]$ 区间之间。

假设有两种数值型特征，$x1$ 的取值范围为 $[0,10]$，$x2$ 的取值范围为 $[0,3]$。在学习速率相同的情况下，$x1$ 的更新速度会慢于 $x2$，故需要较多的迭代才能找到最优解。但是如果将 $x1$ 和 $x2$ 归一化到相同的数值区间后，优化目标的等值图就会变成圆形，$x1$ 和 $x2$ 的更新速度变得更为一致，容易更快地通过梯度下降找到最优解。

# 总结

理论上，一个模型算法拿到训练集所有的特征一起训练模型就要归一化数据。概率模型（如决策树）可以不归一化数据，因为它们不关心变量的值，而是关心<u>变量的分布</u>和<u>变量之间的条件概率</u>。

1. 在分类、聚类算法中，需要使用距离来度量相似性的时候，或者使用PCA技术进行降维的时候，第二种方法（z-score norm）表现更好。
2. 在不涉及距离度量、协方差计算、数据不符合正态分布的时候，可以使用第一种（min-max scaling）或其他归一化方法。

# references

[1] [线性函数归一化（Min-Max Scaling）](https://zhuanlan.zhihu.com/p/375746280)

[2] [机器学习部分：（均值）标准归一化和最大最小（线性）归一化问题](https://blog.csdn.net/wyqwilliam/article/details/81750832)